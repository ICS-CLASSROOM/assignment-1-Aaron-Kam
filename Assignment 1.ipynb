{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: \r\n",
    "* Answer each question in a separate Jupynet Notebook Cell\r\n",
    "* Pleas keep the code in your cells short. \r\n",
    "  * In notebook programming cells are typicaly short to facilitate reading. \r\n",
    "  * If well toughout, most answers in this assignment won're require more than 3 or 4 lines of code. \r\n",
    "* Do no change the list of import, i.e., do not add additional libraries. Those included are the only ones you are allowed to use.\r\n",
    "* Add your first and Last name below:\r\n",
    "\r\n",
    "# Aaron Kam\r\n",
    "    \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "from itertools import product\r\n",
    "from collections import Counter\r\n",
    "from tqdm.notebook import tqdm\r\n",
    "import random\r\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be working with Corona Virus (SARS-CoV2) data that was obtained from the [National Center for Biotechnology Information](https://www.ncbi.nlm.nih.gov/). You will need two files. The first (`data/coronavirus_info.csv`) is small and is provided in the GitHub Repo. The second  (`data_report.jsonl`) is larger so you will need to download a compressed version, which you will need to uncompress prior to using. You can downlod the second file here:\n",
    "\n",
    "https://www.dropbox.com/s/qdn67rshygz06ff/data_report.jsonl.gz?dl=0\n",
    "\n",
    "We start by loading `data/coronavirus_info.csv` (Code provided below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "table = pd.read_csv(\"data/coronavirus_info.csv\", low_memory=False)\r\n",
    "table = table.drop([\"US State\", \"Host Name\", \"Host Taxonomy ID\", \"Sequence Type\", \"Species Taxonomy Id\", \"Nuc Completeness\", \"BioProject\", \"BioSample\"], axis=1)\r\n",
    "\r\n",
    "missing = table[\"Geo Location\"].isnull()\r\n",
    "table.loc[missing, \"Geo Location\"] = \"\"\r\n",
    "\r\n",
    "print(table.shape)\r\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.1\r\n",
    "\r\n",
    "* The location of each of the sequences is recorded under the `Geo Location` column.  How many entries are from Asia?\r\n",
    "  * Note that for some records, the `Geo Location` column is missing\r\n",
    "  * Display the results using the following format: \r\n",
    "    Asia: XXXX,\r\n",
    "    North America': XXXX,\r\n",
    "    Europe: XXXX,\r\n",
    "    Oceania: XXXX,\r\n",
    "    Africa: XXXX,\r\n",
    "    South America: XXXX \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\r\n",
    "continents = ['Asia', 'North America', 'Europe', 'Oceania', 'Africa', 'South America']\r\n",
    "# Use str.contains to make boolean mask then sum\r\n",
    "for c in continents:\r\n",
    "  print(f\"{c}: {table['Geo Location'].str.contains(c).sum()}, \", end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.2\n",
    "Use the `coronavirus_info.csv` table to count the entries that are from Hawaii. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\r\n",
    "print(f\"Hawaii entries: {table['Geo Location'].str.contains('Hawaii').sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.3\r\n",
    "\r\n",
    "The file `data_report.jsonl` contains the variants of the virus in the DB. This `json` file is a list of records (one per line) for each one of the genomes in the database. Before we work with the large file, we will experiment with a file containing a single record.\r\n",
    "\r\n",
    "The file `single_record.json` contains a single sample record. Use the `JSON` library to load the file `single_record.json` into a variable called `sample_vir_record`\r\n",
    "\r\n",
    "1. how many first-level keys does this record have?\r\n",
    "  * Do not count nested keys. Only those at the top level should be counted\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\r\n",
    "with open('data/single_record.json') as sample_vir_file:\r\n",
    "  sample_vir_record = json.load(sample_vir_file)\r\n",
    "\r\n",
    "print(f'First level keys: {len(sample_vir_record.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.4\n",
    "\n",
    "Each Covid various in this database is classified according to a system referred to as the Pangolin (Phylogenetic Assignment of Named Global Outbreak LINeages) classification. It is not essential to complete the assignment that you understand this system, but if you're interested in learning more, see:\n",
    "\n",
    "https://cov-lineages.org/resources/pangolin.html\n",
    "\n",
    "The Pangolin classification of this sample record is nested within the `virus` key:\n",
    "```json\n",
    "{ ...\n",
    "  \"virus\": {\n",
    "              ...\n",
    "              \"pangolinClassification\": \n",
    "              ...\n",
    "            }\n",
    "  ... \n",
    "}\n",
    "```\n",
    "Write code to extract the classification of this record. The result should be `B.1.1.214`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\r\n",
    "sample_vir_record['virus']['pangolinClassification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hear in the news about `Alpha`, `Beta`, `Delta` variants of concern and recently the Mu variant as being of interest. Basedon your answer to `Q.3`, you may have been tempted to infer that this virus is of type `Beta` since the first letter is `B`. In fact, this variant of type `Alpha` and is a variant of concern. Although not relevant to this exercise, the rules for naming new variants and the list of known `SARS-CoV-2` are provided here:\n",
    "\n",
    "https://www.pango.network/how-does-the-system-work/what-are-pango-lineages/\n",
    "\n",
    "https://cov-lineages.org/lineage_list.html\n",
    "\n",
    "\n",
    "The following short video is very helpful for understanding what a variant is, how it arises, how it's named, and why some variants are more concerning than others.\n",
    "\n",
    "https://www.youtube.com/watch?v=B8UEZ9cfgz4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.5\n",
    "\n",
    "Write Python code that counts all the different kinds of variants in `data_report.jsonl`. \n",
    "Because the file is 7GB, you won't likely be able to load it into your laptop's RAM using Python. I encountered this error when trying to open it on my laptop\n",
    "\n",
    "![](https://www.dropbox.com/s/lieo685pafkgm5e/ram_error.png?dl=1)\n",
    "\n",
    "\n",
    "It would be easy to extract the data from the pangolinClassification field of each `json` record by reading each line (a record) at a time.\n",
    "\n",
    "The list of current variants of concern we are interested in are:\n",
    "      * Alpha (B.1.1.7)\n",
    "      * Beta (B.1.351, B.1.351.2, B.1.351.3)\n",
    "      * Delta (B.1.617.2, AY.1, AY.2, AY.3)\n",
    "      * Gamma (P.1, P.1.1, P.1.2) \n",
    "\n",
    "You should get something similar to what follows:\n",
    "```\n",
    "Alpha: X\n",
    "Beta: X\n",
    "Delta: X\n",
    "Gamma: X\n",
    "```\n",
    "Where `X` represents the counts for relevant variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\r\n",
    "# Turn dictionary so it includes\r\n",
    "vars_of_interest = {\r\n",
    "  'Alpha' : {'data' : ['B.1.1.7'], 'count': 0},\r\n",
    "  'Beta' : {'data' : ['B.1.351', 'B.1.351.2', 'B.1.351.3'], 'count': 0},\r\n",
    "  'Gamma' : {'data' : ['B.1.617.2', 'AY.1', 'AY.2', 'AY.3'], 'count': 0},\r\n",
    "  'Delta' : {'data' : ['P.1', 'P.1.1', 'P.1.2'], 'count': 0}}\r\n",
    "\r\n",
    "with open('data/data_report.jsonl', 'r') as report_file:\r\n",
    "  for entry in report_file: # read file line by line\r\n",
    "    record = json.loads(entry)['virus'] # convert string to dictionary object\r\n",
    "    if 'pangolinClassification' in record.keys(): # some records do not have pangolin Classification\r\n",
    "      record = record['pangolinClassification']\r\n",
    "      for letter in vars_of_interest.keys(): # iterate through the variants and add them to the count\r\n",
    "        for variant in vars_of_interest[letter]['data']:\r\n",
    "          vars_of_interest[letter]['count'] += (variant in record)\r\n",
    "\r\n",
    "for letter in vars_of_interest.keys():   \r\n",
    "  print(f\"{letter}: {vars_of_interest[letter]['count']}\")\r\n",
    "\r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find similar viruses.\n",
    "\n",
    "It's often useful to compare viruses to study how similar strains are. While sophisticated algorithms to compare a pair of viruses exist, these are typically computationally intensive and cannot be used to carry out a large number of comparisons. \n",
    "\n",
    "An alternative, albeit less sensitive, approach consists of comparing word counts (called k-mers, where k is the word size) across genomes.  Suppose we have two viruses X and Y, with the following Genomes.\n",
    "```\n",
    "X = \"ACGTAGTGCATGTGTAGCTGTGTAGCTGTAC\"\n",
    "Y = \"ACTAGTGCATGTGTAGCTCTGTAGCTGATAC\"\n",
    "```\n",
    "\n",
    "To compare `X` and `Y`, we first vectorize these genomes by marking the presence of words (k-mers) as a boolean value, 0 if absent and 1 if the word is present. This method assumes that similar genomes will have the same words, which makes sense.\n",
    "\n",
    "This idea, which is referred to as the bag of words model is computationally efficient, making it ideal to vectorize text in big data analytics. Another variant of this model requires replacing the presence and absence by counts for each word.\n",
    "\n",
    "The code below vectorizes an input DNA sequence intro k-mers of size k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\r\n",
    "def get_kmer_2(X):\r\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "    words_size_2 = [\"\".join(dna_prod) for dna_prod in product(DNA, DNA)]\r\n",
    "    counts = pd.Series([0 for _ in words_size_2], index = words_size_2)\r\n",
    "    words_in_X = set([X[i:i+2] for i in range(0, len(X)-1)])\r\n",
    "    counts[list(words_in_X)] = 1\r\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "# X has 3 words of size 2 (AC, CG, GT)\n",
    "X = \"ACGT\"\n",
    "get_kmer_2(\"ACGT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below takes a dictionary of sequences' counts as a `pandas Series` and prints it using HTML Table, which you might agree is nicer to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "def pretty_print_counts(counts_dict):\n",
    "    list_of_count = [data.to_list() for data in counts_dict.values()]\n",
    "    list_of_indices = [x for x in counts_dict.keys()]\n",
    "    list_of_columns = list(counts_dict.values())[0].index.to_list()\n",
    "    df_single_level_cols = pd.DataFrame(list_of_count,\n",
    "                                        index=[x for x in counts_dict.keys()],\n",
    "                                       columns = list_of_columns)    \n",
    "    return df_single_level_cols \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED -- DO NOT REMOVE\n",
    "\n",
    "X = \"ACGTACGTACGTACGT\"\n",
    "Y = \"ACGTACAAACGTACGT\"\n",
    "Z = \"TTTTACAAACGTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer_2(X), \"Y\": get_kmer_2(Y), \"Z\": get_kmer_2(Z)}\n",
    "pretty_print_counts(counts_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.6\n",
    "\n",
    "Write a function that computes the Jaccard similarity between two feature vectors, A and B. If you recall, Jaccard similarity is computed as:\n",
    "\n",
    "$$J(A,B) = \\frac{A \\cap B}{A \\cup B}$$\n",
    "\n",
    "In other words, the number of items shared by `A` and `B` over the set of all items in `A` or `B`.\n",
    "\n",
    "For example, for `A= get_kmer_2(X)` and Y = get_kmer_2(B) above,\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{4}{6}\n",
    "$$\n",
    "\n",
    "Your function should have the following signature: \n",
    "\n",
    "`jaccard(A, B)`\n",
    "\n",
    "Where `A` and `B` are `pandas Series`\n",
    "\n",
    "\n",
    "Test your function using the code below to make sure it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "def jaccard(A,B):\r\n",
    "  intersection = A & B # bitwise intersection\r\n",
    "  union = A | B # bitwise union\r\n",
    "  return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\r\n",
    "A = get_kmer_2(X)\r\n",
    "B = get_kmer_2(Y)\r\n",
    "\r\n",
    "assert jaccard(A, B) == 4/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.7 \n",
    "\n",
    "* Compute the jaccard similarity for the pairs of sequences `(X, Y)`, `(X, Z)`, `(Y, Z)`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "C = get_kmer_2(Z)\r\n",
    "print(f'(X,Y): {jaccard(A, B):.2f}, (X,Z):{jaccard(A, C):.2f}, (Y,Z): {jaccard(B, C):.2f}')\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.8\n",
    "\n",
    "The vectors representing the presence and absence of words in both `Y` and `Z` are very similar (Jaccard = 0.85), despite major differences at the DNA level between these two sequences. This is because the words are small -- it is as if you were comparing a history book with a book on Python using words of size 2. It's very likely that both books will contain the same words of size 2. Increasing the size of `k` will produce substantial differences. \n",
    "\n",
    "Change the function `get_kmer_2` so that given a sequence `X` and a k-mer size `k`, the function returns a boolean vector of all the words of size `k` in `X`. Cal the function `get_kmer`\n",
    "\n",
    "\n",
    "\n",
    "The following code can be used to generate all DNA words of size `k`\n",
    "```pyton\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "```\n",
    "\n",
    "Once done, use the code below to test your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "def get_kmer(X,k):\r\n",
    "  DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "  words_size_k = [\"\".join(dna_prod) for dna_prod in product(*([DNA]*k))]\r\n",
    "  #print(words_size_k)\r\n",
    "  counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\r\n",
    "  words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\r\n",
    "  counts[list(words_in_X)] = 1\r\n",
    "  return counts\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST PROVIDED -- DO NOT REMOVE\n",
    "X = \"ACGTGATGATTG\"\n",
    "\n",
    "counts = get_kmer(X, k=1)\n",
    "assert counts.tolist() == [1,1,1,1]\n",
    "\n",
    "\n",
    "counts = get_kmer(X, k=3)\n",
    "assert (counts[[\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]] == 1).sum()  == 8\n",
    "assert (counts.drop([\"ACG\", \"CGT\", \"GTG\", \"TGA\", \"GAT\", \"ATG\", \"ATT\", \"TTG\"]) == 0).sum()  == 56\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.9\n",
    "\n",
    "* Compute the Jaccard similarity for the pairs `(X, Y)`, `(X, Z)`, `(Y, Z)` using `k= 5`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "print(f'(X,Y): {jaccard(get_kmer(X, k=5), get_kmer(Y, k=5)):.2f}, (X,Z):{jaccard(get_kmer(X, k=5), get_kmer(Z, k=5)):.2f}, (Y,Z): {jaccard(get_kmer(Y, k=5), get_kmer(Z, k=5)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appropriate word size varies with the length of the text, with larger words depicting similarity more accurately. However, large values of `k` are:\r\n",
    "1. More computationally intensive to compute. With k = 12, there are $4^{12} = 16,777,216$ words to compute for each sequence.\r\n",
    "\r\n",
    "2. More likely to skew the distance between fairly similar sequences. For example `k=8`, the Jaccard index between `X` and `Y` is `0`, even though `X` and `Y` have only two mismatching characters. While this is an extreme case due to the fact that X and Y are short, the logic applies to longer sequences and larger values of `k`\r\n",
    "\r\n",
    "\r\n",
    "![](https://www.dropbox.com/s/rhw5szbiohsqu7w/mismatches.png?dl=1)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "print('so what is the question?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code I used is provided as a reference below. The code took 7 hours to complete on a single machine and approximately 12 minutes on a larger server with 72 cores and 1TB of RAM. To parallelize the execution, I split the file into files that contain 1000 sequences each and used GNU Parallel to run each file on a single CPU core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE PROVIDED FOR ILLUTRATION -- DO NOT REMOVE\n",
    "# RUNNING LOCALLY MAY TAKE A LONG TIME TO COMPLETE\n",
    "\n",
    "# k = 8 \n",
    "# DNA = [\"A\", \"C\", \"G\", \"T\"]\n",
    "# words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\n",
    "\n",
    "    \n",
    "# def get_kmer_mod(X):\n",
    "#     counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\n",
    "#     words_in_X = set([X[i:i+k] for i in range(0, len(X)-k+1)])\n",
    "#     counts[list(words_in_X)] = 1\n",
    "#     return counts   \n",
    "\n",
    "# def replace_bad_nucs(seq):\n",
    "#     for character in ['W', 'K', \"Y\", \"M\", 'H']:\n",
    "#         seq = seq.replace(character, 'A') \n",
    "        \n",
    "#     for character in ['R', 'S', 'D', \"V\", \"B\"]:\n",
    "#         seq = seq.replace(character, 'C') \n",
    "        \n",
    "#     seq = seq.replace(\"N\", '') \n",
    "    \n",
    "#     return seq\n",
    "\n",
    "# all_counts = []\n",
    "# all_names = []\n",
    "# with tqdm(total=1000) as pbar:\n",
    "#     for record in SeqIO.parse(\"myseq0.fa\", 'fasta'):\n",
    "#         all_names.append(record.id)\n",
    "#         seq = replace_bad_nucs(str(record.seq))\n",
    "\n",
    "#         counts = get_kmer_mod(seq)\n",
    "#         all_counts.append(counts)\n",
    "#         pbar.update(1)\n",
    "    \n",
    "# kmer_counts = pd.DataFrame(all_counts, index = all_names)\n",
    "# kmer_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Sequences\n",
    "\n",
    "We are interested in finding pairs of sequences that are very similar. However, comparing the sequences pairwise is not tractable since it would require carrying out $429282 * (429282 - 1) / 2 = 92_141_303_121$ comparisons.\n",
    "\n",
    "Instead, we will use the hashing-based approach covered in class. Rather than hashing a sequence over all k-mers, we will only compute the hash for a subset of k-mers. we will repeat the operation n times to avoid that similar sequences are assigned to different bins due to a single, rare mismatch.\n",
    "\n",
    "This, as discussed in class, is computationally more efficient compared to computing all pairwise sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.10 \r\n",
    "\r\n",
    "Write a function that takes a `pandas  Series` and a subset of columns and returns the hash computed on the subset of columns. Call this function`hash_subset`.\r\n",
    "\r\n",
    "As an example, consider all words with a size of 2 as follows \r\n",
    "\r\n",
    "|\t|AA\t|AC\t|AG\t|AT\t|CA | CC| CG| CT| GA| GC| GG| GT| TA| TC| TG| TT|\r\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\r\n",
    "| A\t|0\t|1\t|0\t|0\t|0\t|0\t|1\t|0\t| 0 |0\t|0\t| 1 |1  |0\t|0\t|0  |\r\n",
    "\r\n",
    "```python\r\n",
    "hash_on_subset(A, [\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"]) \r\n",
    "```\r\n",
    "\r\n",
    "is equivalent to:\r\n",
    "\r\n",
    "```python\r\n",
    "hash((1, 1, 0, 1, 1)) == 5085477689562523216\r\n",
    "```\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write you code here\r\n",
    "def hash_subset(A,ls):\r\n",
    "  # get value of rows with indicies that are in the list\r\n",
    "  return hash(tuple(A.loc[A.index.isin(ls)].values))\r\n",
    "\r\n",
    "A = pd.Series((0,1,0,0,0,0,1,0,0,0,0,1,1,0,0,0), index=('AA','AC','AG','AT','CA','CC','CG','CT', 'GA','GC','GG','GT','TA','TC','TG','TT'))\r\n",
    "hash_subset(A,[\"AC\", \"CG\", \"CT\", \"GT\", \"TA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The method `sample` from the random module `m` words from a list\n",
    "\n",
    "For example, running:\n",
    "```python\n",
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )\n",
    "```\n",
    "returns\n",
    "```\n",
    "['A', 'C']\n",
    "```\n",
    "The returned subset may be different for you.\n",
    "\n",
    "* The code below randomly selects `m=20` k-mers we will use to compare the genomes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample( [\"A\", \"C\", \"G\", \"T\"], 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=8\r\n",
    "DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "\r\n",
    "m=20\r\n",
    "subset_kmers = random.sample(words_size_k, m)\r\n",
    "\r\n",
    "subset_kmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.12\n",
    "\n",
    "\n",
    "Apply the function `hash_subset` to all the rows of `all_kmers_df`. The data science (*vectorized*) way to do so is using the `apply` method available on a `pandas DataFrame` instead of using for loops. For example, given a DataFrame `df` such that:\n",
    "\n",
    "```\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\n",
    "\n",
    "```\n",
    "then \n",
    "```\n",
    "df.apply(max, args=[] axis=1)\n",
    "```\n",
    "applies the `max()` function on each row (`axis = 1`). Here, `args` is empty since `max` does not take any additional arguments.\n",
    "\n",
    "The example below shows how to use `apply` when the function requires additional arguments. In this example, we apply a function that sums all the values of a row and adds to the sum an offset (2 by default)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of rows + an offset of 5 is:\n",
      "0    11\n",
      "1    20\n",
      "dtype: int64\n",
      "The sum of rows + an offset of 10 is:\n",
      "0    16\n",
      "1    25\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE CODE PROVIDED -- DO NOT REMOVE\r\n",
    "def add_val_to_sum(x, offset=2):\r\n",
    "    return x.sum() + offset\r\n",
    "    \r\n",
    "df = pd.DataFrame([[1,2,3], [4,5,6]])\r\n",
    "\r\n",
    "print(\"The sum of rows + an offset of 5 is:\")\r\n",
    "print(df.apply(add_val_to_sum, args=[5], axis=1))\r\n",
    "\r\n",
    "print(\"The sum of rows + an offset of 10 is:\")\r\n",
    "print(df.apply(add_val_to_sum, args=[10], axis=1))\r\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kmers_df = pd.read_csv('data/all_kmers_10k.csv', index_col=0)\r\n",
    "all_kmers_df.apply(hash_subset, args=[all_kmers_df.columns], axis=1)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q.13\n",
    "\n",
    "\n",
    "Use `apply()` to apply `hash_subset` and compute the hash values for all the rows of `all_kmers_df` over `subset_kmers`\n",
    "\n",
    "* Create a dict by parsing the results to group sequences that yield the same hash under the same bins. Each key in the dict should be a key and each value is a list of sequences that have the same value.\n",
    "\n",
    "For example, in the dictionary below, X and Y have the same hash value (123456) over a given subset of kmers, whereas Z has a different hash over the same subsets.\n",
    "\n",
    "```\n",
    "{\"123456\": [X,Y], \"654321\": [Z]}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_kmers_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20812/1645344172.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Write your code here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msubset_hashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_kmers_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_subset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubset_kmers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Swap values and indicies then groupby hash (index), aggregate names into lists, then send to dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_hashes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubset_hashes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_kmers_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code here\r\n",
    "subset_hashes = all_kmers_df.apply(hash_subset, args=[subset_kmers], axis=1)\r\n",
    "# Swap values and indicies then groupby hash (index), aggregate names into lists, then send to dictionary\r\n",
    "pd.Series(subset_hashes.index.values, index=subset_hashes).groupby(level=0).agg(list).to_dict()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q. 15\n",
    "\n",
    "Here we use the presence and absence of words, i.e., a vector of booleans, to encode a sequence. The problem with this approach is that it considers the sequences to be identical, even if their word counts differ substantially. For example, given the sequence `X`, `Y` and `Z` as follows\n",
    "```\n",
    "X = ATAGATAGATAGATAGATT\n",
    "Y = ATAGATAGATAGATAGATT\n",
    "Z = ATAGATTTTTTTTTTTTTT\n",
    "```\n",
    "With k =2, all three sequences mach on their vector of word presence/absense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"ATAGATAGATAGATAGATT\"\n",
    "Y = \"ATAGATAGATAGATAGATT\"\n",
    "Z = \"ATAGATTTTTTTTTTTTTT\"\n",
    "\n",
    "counts_dict = {\"X\": get_kmer(X, k=2), \"Y\": get_kmer(Y, k=2), \"Z\": get_kmer(Z, k=2)}\n",
    "pretty_print_counts(counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comparing these sequences based on word counts, X is much more similar to Y than it is to Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kmer_counts(X, k):\r\n",
    "    DNA = [\"A\", \"C\", \"G\", \"T\"]\r\n",
    "    words_size_k = [\"\".join(prod) for prod in product(*([DNA]*k))]\r\n",
    "    counts = pd.Series([0 for _ in words_size_k], index = words_size_k)\r\n",
    "    counts_words_in_x = Counter([X[i:i+k] for i in range(0, len(X)-k+1)])\r\n",
    "    counts.update(counts_words_in_x)\r\n",
    "    return counts\r\n",
    "\r\n",
    "\r\n",
    "counts_dict = {\"X\": get_kmer_counts(X, k=2), \"Y\": get_kmer_counts(Y, k=2), \"Z\": get_kmer_counts(Z, k=2)}\r\n",
    "pretty_print_counts(counts_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given an example (vectors) to justify why hashing is not ideal with counts.\n",
    "  * Use any means you think are useful to illustrate your point (e.g.: figure, simulation (yes, please!))\n",
    " \n",
    "* Describe how the random project approach discussed in class can help solve the issue discussed\n",
    "  * Use code to illustrate how random projection works in the following example.\n",
    "    * I.e., provide code to provide an example where `X` and `Y` are assigned to the same bin and `Y` is assigned to a different bin.\n",
    "    * You can choose any vector values as needed \n",
    " \n",
    "```python\n",
    "X = [1,2]\n",
    "Y = [2,2]\n",
    "Z = [5,1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here\r\n",
    "def simulate_points(n,dim):\r\n",
    "  points = [tuple([random.randrange(0,100) for j in range(dim)]) for i in range(n)] # generate n random points with specified dimensions\r\n",
    "  \r\n",
    "  hash_dists = []\r\n",
    "  indicies = []\r\n",
    "  for i in range(len(points)):\r\n",
    "    for j in range(i,len(points)):\r\n",
    "      if i != j:\r\n",
    "        eu_dist = abs(sum([(points[i][_] - points[j][_])**2 for _ in range(len(points[i]))])**(1/2))\r\n",
    "        man_dist = (sum([abs(points[i][_] - points[j][_]) for _ in range(len(points[i]))]))\r\n",
    "        hash_dist = abs((hash(points[i])-hash(points[j])))\r\n",
    "\r\n",
    "        indicies.append((points[i],points[j]))\r\n",
    "        hash_dists.append((eu_dist, man_dist, hash_dist))\r\n",
    "\r\n",
    "  return pd.DataFrame(hash_dists, columns=['euclid dist', 'man_dist', 'hash dist'], index=pd.MultiIndex.from_tuples(indicies, names=[\"first\", \"second\"]))\r\n",
    "print('The table below shows the hashing distances between the points in descending order. If hashes were able to show proximity then the euclidean distances should also be in descending order but are not. Therefore hashing is not suitable for showing proximity.')\r\n",
    "simulate_points(5,3).sort_values('hash dist', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The magnitude of the projection  1.8286707459922102\n",
      "The ratio of the projection in terms of D is 0.4954128440366973\n",
      "Projection occurs in bin 0.0 \n",
      "\n",
      "The magnitude of the projection  2.7091418459143854\n",
      "The ratio of the projection in terms of D is 0.7339449541284404\n",
      "Projection occurs in bin 0.0 \n",
      "\n",
      "The magnitude of the projection  4.876455322645894\n",
      "The ratio of the projection in terms of D is 1.3211009174311927\n",
      "Projection occurs in bin 1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "X = [1,2]\r\n",
    "Y = [2,2]\r\n",
    "Z = [5,1]\r\n",
    "\r\n",
    "ref = [3.25,1.75]\r\n",
    "\r\n",
    "amp_ref = (ref[0]**2 + ref[1]**2)**(1/2)\r\n",
    "  \r\n",
    "X_dot_ref = X[0]*ref[0] + X[1]*ref[1]\r\n",
    "Y_dot_ref = Y[0]*ref[0] + Y[1]*ref[1]\r\n",
    "Z_dot_ref = Z[0]*ref[0] + Z[1]*ref[1]\r\n",
    "\r\n",
    "proj_X = X_dot_ref / amp_ref\r\n",
    "proj_Y = Y_dot_ref / amp_ref\r\n",
    "proj_Z = Z_dot_ref / amp_ref\r\n",
    "\r\n",
    "print (f\"The magnitude of the projection  {proj_X}\")\r\n",
    "print(f\"The ratio of the projection in terms of D is {proj_X/amp_ref}\")\r\n",
    "print(f\"Projection occurs in bin {proj_X//amp_ref} \\n\")\r\n",
    "\r\n",
    "print (f\"The magnitude of the projection  {proj_Y}\")\r\n",
    "print(f\"The ratio of the projection in terms of D is {proj_Y/amp_ref}\")\r\n",
    "print(f\"Projection occurs in bin {proj_Y//amp_ref} \\n\")\r\n",
    "\r\n",
    "print (f\"The magnitude of the projection  {proj_Z}\")\r\n",
    "print(f\"The ratio of the projection in terms of D is {proj_Z/amp_ref}\")\r\n",
    "print(f\"Projection occurs in bin {proj_Z//amp_ref} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20812/3680184297.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ark47\\AppData\\Local\\Temp/ipykernel_20812/3680184297.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    rand_set.loc[,1]\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82b5efee887e002da33821188ce89fd9ea39794247391eab730cd4cb3f48fced"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}